{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85806a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 22:45:47.302255: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-05 22:45:47.819925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751735748.000096   71420 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751735748.051571   71420 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751735748.540273   71420 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751735748.540597   71420 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751735748.540608   71420 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751735748.540614   71420 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-05 22:45:48.597990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1us/step\n",
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3a9f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a81800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ea1296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 129ms/step - accuracy: 0.1127 - loss: 2.2986 - val_accuracy: 0.2863 - val_loss: 2.2489\n",
      "Epoch 2/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 136ms/step - accuracy: 0.2384 - loss: 2.2409 - val_accuracy: 0.5448 - val_loss: 2.1776\n",
      "Epoch 3/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 133ms/step - accuracy: 0.3692 - loss: 2.1727 - val_accuracy: 0.6386 - val_loss: 2.0803\n",
      "Epoch 4/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 120ms/step - accuracy: 0.4718 - loss: 2.0748 - val_accuracy: 0.6923 - val_loss: 1.9411\n",
      "Epoch 5/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 138ms/step - accuracy: 0.5465 - loss: 1.9390 - val_accuracy: 0.7346 - val_loss: 1.7516\n",
      "Epoch 6/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 121ms/step - accuracy: 0.6098 - loss: 1.7557 - val_accuracy: 0.7695 - val_loss: 1.5171\n",
      "Epoch 7/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 123ms/step - accuracy: 0.6495 - loss: 1.5486 - val_accuracy: 0.7950 - val_loss: 1.2688\n",
      "Epoch 8/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 142ms/step - accuracy: 0.6733 - loss: 1.3396 - val_accuracy: 0.8130 - val_loss: 1.0500\n",
      "Epoch 9/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 127ms/step - accuracy: 0.7010 - loss: 1.1581 - val_accuracy: 0.8268 - val_loss: 0.8826\n",
      "Epoch 10/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 136ms/step - accuracy: 0.7210 - loss: 1.0269 - val_accuracy: 0.8367 - val_loss: 0.7633\n",
      "Epoch 11/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 135ms/step - accuracy: 0.7355 - loss: 0.9282 - val_accuracy: 0.8457 - val_loss: 0.6781\n",
      "Epoch 12/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 134ms/step - accuracy: 0.7539 - loss: 0.8525 - val_accuracy: 0.8529 - val_loss: 0.6157\n",
      "Epoch 13/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 129ms/step - accuracy: 0.7670 - loss: 0.7874 - val_accuracy: 0.8599 - val_loss: 0.5685\n",
      "Epoch 14/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 131ms/step - accuracy: 0.7748 - loss: 0.7425 - val_accuracy: 0.8658 - val_loss: 0.5319\n",
      "Epoch 15/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 136ms/step - accuracy: 0.7863 - loss: 0.7134 - val_accuracy: 0.8712 - val_loss: 0.5029\n",
      "Epoch 16/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 130ms/step - accuracy: 0.7968 - loss: 0.6795 - val_accuracy: 0.8759 - val_loss: 0.4788\n",
      "Epoch 17/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 132ms/step - accuracy: 0.8035 - loss: 0.6524 - val_accuracy: 0.8803 - val_loss: 0.4582\n",
      "Epoch 18/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 132ms/step - accuracy: 0.8134 - loss: 0.6222 - val_accuracy: 0.8841 - val_loss: 0.4406\n",
      "Epoch 19/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - accuracy: 0.8136 - loss: 0.6120 - val_accuracy: 0.8877 - val_loss: 0.4258\n",
      "Epoch 20/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - accuracy: 0.8235 - loss: 0.5839 - val_accuracy: 0.8897 - val_loss: 0.4123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
    "print(\"The model has successfully trained\")\n",
    "\n",
    "model.save('mnist.h5')\n",
    "print(\"Saving the model as mnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34b2dd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4122947156429291\n",
      "Test accuracy: 0.8896999955177307\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a792cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Predicted Digit: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load trained MNIST model\n",
    "model = load_model('mnist.h5')\n",
    "\n",
    "# Load your image (change the filename accordingly)\n",
    "img = cv2.imread('xoxo.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Resize to 28x28\n",
    "img = cv2.resize(img, (28, 28))\n",
    "\n",
    "# Invert colors (MNIST is white digits on black background)\n",
    "img = 255 - img\n",
    "\n",
    "# Normalize to 0-1\n",
    "img = img / 255.0\n",
    "\n",
    "# Reshape to match model input (1 sample, 28x28, 1 channel)\n",
    "img = img.reshape(1, 28, 28, 1)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img)\n",
    "predicted_digit = np.argmax(prediction)\n",
    "\n",
    "print(f'Predicted Digit: {predicted_digit}')\n",
    "\n",
    "# Show the image with prediction\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.title(f'Prediction: {predicted_digit}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884e341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Predicted Digit: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('mnist.h5')\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread('d.jpeg', cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "img = cv2.resize(img, (28, 28))  # Resize to 28x28 pixels\n",
    "img = img.astype('float32') / 255  # Normalize to [0, 1]\n",
    "img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "img = np.expand_dims(img, axis=-1)  # Add channel dimension (for grayscale)\n",
    "\n",
    "# Predict the digit\n",
    "prediction = model.predict(img)\n",
    "predicted_digit = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted Digit:\", predicted_digit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00cb660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 11:55:04.990771: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-11 11:55:05.364505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752215105.551445  436344 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752215105.599585  436344 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752215105.987134  436344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752215105.987210  436344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752215105.987212  436344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752215105.987213  436344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 11:55:06.020520: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-11 11:55:17.473613: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-07-11 11:55:18.437 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-11 11:55:18.769 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/vasanth/.local/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-07-11 11:55:18.773 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-11 11:55:18.775 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-11 11:55:18.776 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-11 11:55:18.778 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-11 11:55:18.779 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-11 11:55:18.780 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-11 11:55:18.783 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-11 11:55:18.785 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Load trained MNIST model\n",
    "model = load_model('mnist.h5')\n",
    "\n",
    "st.title(\"Handwritten Digit Recognition\")\n",
    "st.write(\"Upload an image of a handwritten digit (MNIST-style, grayscale)\")\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Load image\n",
    "    image = Image.open(uploaded_file).convert('L')  # convert to grayscale\n",
    "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
    "    \n",
    "    # Preprocess image\n",
    "    img = np.array(image)\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img = 255 - img  # invert colors if needed\n",
    "    img = img / 255.0  # normalize\n",
    "    img = img.reshape(1, 28, 28, 1)\n",
    "    \n",
    "    # Prediction\n",
    "    prediction = model.predict(img)\n",
    "    predicted_digit = np.argmax(prediction)\n",
    "    \n",
    "    st.subheader(f'Predicted Digit: {predicted_digit}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
